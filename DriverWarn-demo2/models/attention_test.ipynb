{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image,ImageDraw\n",
    "from PIL import ImageFont\n",
    "\n",
    "import tensorflow as tf # TF2\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "  with open(filename, 'r') as f:\n",
    "    return [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    face_model = \"./face_landmark.tflite\"\n",
    "    iris_model = \"./iris_landmark.tflite\"\n",
    "    landmark_file = \"./landmark_contours.txt\"\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=face_model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    iris_interpreter = tf.lite.Interpreter(model_path=iris_model)\n",
    "    iris_interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    print(\"face landmark input info:\",input_details)\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    iris_input = iris_interpreter.get_input_details()\n",
    "    print(\"iris landmark input info:\",iris_input)\n",
    "    iris_output = iris_interpreter.get_output_details()\n",
    "\n",
    "    # check the type of the input tensor\n",
    "    floating_model = input_details[0]['dtype'] == np.float32\n",
    "    iris_float_model = iris_input[0][\"dtype\"] == np.float32\n",
    "\n",
    "    # NxHxWxC, H:1, W:2\n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "\n",
    "    iris_height = iris_input[0][\"shape\"][1]\n",
    "    iris_width = iris_input[0][\"shape\"][2]\n",
    "\n",
    "    print(\"height = \",height,\"width = \",width)\n",
    "    img = Image.open(\"./img/gst_zero_33.png\").resize((width, height))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    print(\"numpy img size = \",img.size)\n",
    "\n",
    "    eye_points = open(landmark_file,\"r\")\n",
    "    left_eye_points = []\n",
    "    right_eye_points = []\n",
    "    lips_points = []\n",
    "    for line in eye_points:\n",
    "        points = line.split(\" \")\n",
    "        if points[0] == \"left_eye\":\n",
    "            for data in points[1:-1]:\n",
    "                left_eye_points.append(int(data))\n",
    "        elif points[0] == \"right_eye\":\n",
    "            for data in points[1:-1]:\n",
    "                right_eye_points.append(int(data))\n",
    "        elif points[0] == \"lips\":\n",
    "            for data in points[1:-1]:\n",
    "                lips_points.append(int(data))\n",
    "\n",
    "    print(\"left eye points \",left_eye_points)\n",
    "    print(\"right eye points \",right_eye_points)\n",
    "\n",
    "    image = img\n",
    "    if(image.mode == str('RGBA')):\n",
    "        print(\"rgba show\\n\")\n",
    "        r, g, b, a = image.split()\n",
    "        img = Image.merge(\"RGB\", (r, g, b))\n",
    "        image = img\n",
    "\n",
    "    image.show(\"rgb show\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "\n",
    "    # add N dim\n",
    "    input_data = np.expand_dims(img, axis=0)\n",
    "\n",
    "    if floating_model:\n",
    "        input_data = (np.float32(input_data) - 128) / 256 \n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    results = np.squeeze(output_data)\n",
    "\n",
    "    output_data1= interpreter.get_tensor(output_details[1][\"index\"])\n",
    "    print(\"output data 1 = \",output_data1)\n",
    "\n",
    "    print (\"result size = \",len(results))\n",
    "    output_file = open(\"point_file.txt\",\"w\")\n",
    "    point_list = []\n",
    "    color_list = [0,0,0]\n",
    "    left_eye_list = [[],[]]\n",
    "    right_eye_list = [[],[]]\n",
    "    left_eye_center = [0,0]\n",
    "    right_eye_center = [0,0]\n",
    "    for index in range(int(len(results)/3)):\n",
    "        point = (results[3*index],results[3*index + 1])\n",
    "        if index in left_eye_points:\n",
    "            draw.point(point,(255,0,0))\n",
    "            # draw.text([point[0],point[1]],str(index),fill=(100,185,179),)\n",
    "            #print(index,point,\"\\n\")\n",
    "            left_eye_list[0].append(point[0])\n",
    "            left_eye_list[1].append(point[1])\n",
    "        elif index in right_eye_points:\n",
    "            draw.point(point,(255,0,0))\n",
    "            #print(\"right:\",index,point,\"\\n\")\n",
    "            right_eye_list[0].append(point[0])\n",
    "            right_eye_list[1].append(point[1])\n",
    "        elif index in lips_points:\n",
    "            draw.point(point,(255,0,0))\n",
    "            print(\"lips:\",index,point,\"\\n\")\n",
    "            right_eye_list[0].append(point[0])\n",
    "            right_eye_list[1].append(point[1])\n",
    "        else :\n",
    "            draw.point(point,(0,255,0))\n",
    "        # draw.point(point,tuple( color_list))\n",
    "        point_list.append(point)\n",
    "        output_file.writelines(str(point))\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "    left_v_point_bot = [(results[3*145]+results[3*153])/2,(results[3*145+1]+results[3*153+1])/2]\n",
    "    left_v_point_top = [(results[3*159]+results[3*158])/2,(results[3*159+1]+results[3*158+1])/2]\n",
    "\n",
    "    left_v_point_left = [results[3*33],results[3*33+1]]\n",
    "    left_v_point_right = [results[3*133],results[3*133+1]]\n",
    "\n",
    "    EAR = ((left_v_point_bot[1]) - (left_v_point_top[1]) )/(left_v_point_right[0]-left_v_point_left[0])\n",
    "    print(\"EAR value is \",EAR)\n",
    "\n",
    "\n",
    "    lips_distance_h = results[3*409] - results[3*185]\n",
    "    lips_distance_v1 = results[3*84 +1] - results[3*37 +1]\n",
    "    lips_distance_v2 = results[3*17 +1] - results[3*0 +1]\n",
    "    lips_distance_v3 = results[3*314 +1] - results[3*267 +1]\n",
    "\n",
    "    lips_ear = (lips_distance_v1 + lips_distance_v2 + lips_distance_v3) / (3 * lips_distance_h)\n",
    "\n",
    "    print(\"LIPS EAR value is \",lips_ear)\n",
    "\n",
    "    print(\"left eye center count\",left_eye_list)\n",
    "    left_eye_center[1] = sum(left_eye_list[1])/len(left_eye_list[1])\n",
    "    left_eye_center[0] = sum(left_eye_list[0])/len(left_eye_list[0])\n",
    "\n",
    "    right_eye_center[1] = sum(right_eye_list[1])/len(right_eye_list[1])\n",
    "    right_eye_center[0] = sum(right_eye_list[0])/len(right_eye_list[0])\n",
    "    print(\"left eye center\",left_eye_center)\n",
    "    draw.point(left_eye_center,(0,0,255))\n",
    "    draw.point(right_eye_center,(0,0,255))\n",
    "    # draw.point((71.96,91.39),(0,0,255))\n",
    "    # draw.point(point_list,(0,255,0))\n",
    "\n",
    "    left_eye_box = (left_eye_center[0]-32,left_eye_center[1]-32,left_eye_center[0]+32,left_eye_center[1]+32)\n",
    "    right_eye_box = (right_eye_center[0]-32,right_eye_center[1]-32,right_eye_center[0]+32,right_eye_center[1]+32)\n",
    "    iris_left_img = Image.open(\"./img/face.jpeg\").resize((width, height)).crop(left_eye_box)\n",
    "    iris_right_img = Image.open(\"./img/face.jpeg\").resize((width, height)).crop(right_eye_box)\n",
    "    img.show(\"sources img\")\n",
    "\n",
    "    # add N dim\n",
    "    iris_input_data = np.expand_dims(iris_left_img, axis=0)\n",
    "\n",
    "    if iris_float_model:\n",
    "        iris_input_data = (np.float32(iris_input_data) - 128) / 256 \n",
    "\n",
    "    iris_interpreter.set_tensor(iris_input[0]['index'], iris_input_data)\n",
    "\n",
    "    iris_interpreter.invoke()\n",
    "\n",
    "    iris_output_data = iris_interpreter.get_tensor(iris_output[0]['index'])\n",
    "    iris_results = np.squeeze(iris_output_data)\n",
    "\n",
    "    iris_output_data1= iris_interpreter.get_tensor(iris_output[1][\"index\"])\n",
    "    print(\"iris data 1 = \",iris_output_data1)\n",
    "    iris_results = np.squeeze(iris_output_data1)\n",
    "    \n",
    "    iris_draw = ImageDraw.Draw(iris_left_img)\n",
    "    for index in range(int(len(iris_results)/3)):\n",
    "        point = (iris_results[3*index],iris_results[3*index + 1])\n",
    "        iris_draw.point(point,(0,0,255))\n",
    "        iris_draw.text([point[0],point[1]],str(index),fill=(100,185,179))\n",
    "    iris_left_img.show(\"left eye img\")\n",
    "    \n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "face landmark input info: [{'name': 'input_1', 'index': 0, 'shape': array([  1, 192, 192,   3], dtype=int32), 'shape_signature': array([  1, 192, 192,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "iris landmark input info: [{'name': 'input_1', 'index': 0, 'shape': array([ 1, 64, 64,  3], dtype=int32), 'shape_signature': array([ 1, 64, 64,  3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "height =  192 width =  192\n",
      "numpy img size =  (192, 192)\n",
      "left eye points  [33, 7, 7, 163, 163, 144, 144, 145, 145, 153, 153, 154, 154, 155, 155, 133, 33, 246, 246, 161, 161, 160, 160, 159, 159, 158, 158, 157, 157, 173, 173]\n",
      "right eye points  [263, 249, 249, 390, 390, 373, 373, 374, 374, 380, 380, 381, 381, 382, 382, 362, 263, 466, 466, 388, 388, 387, 387, 386, 386, 385, 385, 384, 384, 398, 398]\n",
      "output data 1 =  [[[[14.118195]]]]\n",
      "result size =  1404\n",
      "lips: 0 (95.54586, 123.41642) \n",
      "\n",
      "lips: 13 (96.04622, 128.90251) \n",
      "\n",
      "lips: 14 (96.26972, 144.72456) \n",
      "\n",
      "lips: 17 (96.806526, 153.12823) \n",
      "\n",
      "lips: 37 (89.46097, 122.50287) \n",
      "\n",
      "lips: 39 (83.8483, 124.56502) \n",
      "\n",
      "lips: 40 (81.40119, 127.124) \n",
      "\n",
      "lips: 61 (78.61438, 133.64972) \n",
      "\n",
      "lips: 78 (81.80502, 133.47551) \n",
      "\n",
      "lips: 80 (85.86601, 129.71817) \n",
      "\n",
      "lips: 81 (88.276115, 128.95676) \n",
      "\n",
      "lips: 82 (91.746895, 128.71233) \n",
      "\n",
      "lips: 84 (89.90148, 151.642) \n",
      "\n",
      "lips: 87 (91.48524, 143.85318) \n",
      "\n",
      "lips: 88 (85.52356, 139.91716) \n",
      "\n",
      "lips: 91 (81.426476, 142.89139) \n",
      "\n",
      "lips: 95 (83.964066, 137.64159) \n",
      "\n",
      "lips: 146 (79.751366, 138.1392) \n",
      "\n",
      "lips: 178 (88.00797, 142.15717) \n",
      "\n",
      "lips: 181 (84.79175, 147.98743) \n",
      "\n",
      "lips: 185 (79.61319, 129.81967) \n",
      "\n",
      "lips: 191 (83.98687, 130.8184) \n",
      "\n",
      "lips: 267 (101.85389, 123.05228) \n",
      "\n",
      "lips: 269 (108.043396, 125.5427) \n",
      "\n",
      "lips: 270 (111.40127, 128.41516) \n",
      "\n",
      "lips: 291 (115.46358, 135.23022) \n",
      "\n",
      "lips: 308 (111.807495, 134.76416) \n",
      "\n",
      "lips: 310 (107.062836, 130.60716) \n",
      "\n",
      "lips: 311 (104.22033, 129.63448) \n",
      "\n",
      "lips: 312 (100.51645, 129.06944) \n",
      "\n",
      "lips: 314 (103.68689, 152.22719) \n",
      "\n",
      "lips: 317 (101.351776, 144.27173) \n",
      "\n",
      "lips: 318 (108.040726, 140.9336) \n",
      "\n",
      "lips: 321 (112.63887, 144.28809) \n",
      "\n",
      "lips: 324 (109.70071, 138.77469) \n",
      "\n",
      "lips: 375 (114.495995, 139.6321) \n",
      "\n",
      "lips: 402 (105.21597, 142.93433) \n",
      "\n",
      "lips: 405 (109.13983, 149.07777) \n",
      "\n",
      "lips: 409 (113.93412, 131.31978) \n",
      "\n",
      "lips: 415 (109.41241, 131.88213) \n",
      "\n",
      "EAR value is  0.32442270835781345\n",
      "LIPS EAR value is  0.8549288027695671\n",
      "left eye center count [[62.37201, 60.732155, 79.561424, 66.85752, 70.62114, 74.00462, 76.93833, 78.69993, 76.43438, 73.03638, 69.457954, 65.74382, 63.306232, 64.26123, 78.570045, 61.867558], [80.58897, 78.9078, 81.4615, 82.70903, 83.243965, 83.0417, 82.36933, 81.85163, 78.60304, 77.31407, 76.754326, 76.96499, 77.74301, 81.75008, 80.4167, 78.36512]]\n",
      "left eye center [70.15404558181763, 80.13032913208008]\n",
      "iris data 1 =  [[33.63157   41.213398  -1.1298507 38.68338   41.342354  -1.1640015\n",
      "  33.745277  36.342678  -1.1508325 28.41383   41.07459   -1.1009293\n",
      "  33.498714  46.19247   -1.1256413]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image,ImageDraw\n",
    "from PIL import ImageFont\n",
    "\n",
    "import tensorflow as tf # TF2\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "  with open(filename, 'r') as f:\n",
    "    return [line.strip() for line in f.readlines()]\n",
    "\n",
    "SSD_OPTIONS_FRONT = {\n",
    "    'num_layers': 4,\n",
    "    'input_size_height': 128,\n",
    "    'input_size_width': 128,\n",
    "    'anchor_offset_x': 0.5,\n",
    "    'anchor_offset_y': 0.5,\n",
    "    'strides': [8, 16, 16, 16],\n",
    "}\n",
    "\n",
    "def _ssd_generate_anchors(opts: dict) -> np.ndarray:\n",
    "    \"\"\"This is a trimmed down version of the C++ code; all irrelevant parts\n",
    "    have been removed.\n",
    "    (reference: mediapipe/calculators/tflite/ssd_anchors_calculator.cc)\n",
    "    \"\"\"\n",
    "    layer_id = 0\n",
    "    num_layers = opts['num_layers']\n",
    "    strides = opts['strides']\n",
    "    assert len(strides) == num_layers\n",
    "    input_height = opts['input_size_height']\n",
    "    input_width = opts['input_size_width']\n",
    "    anchor_offset_x = opts['anchor_offset_x']\n",
    "    anchor_offset_y = opts['anchor_offset_y']\n",
    "    anchors = []\n",
    "    while layer_id < num_layers:\n",
    "        last_same_stride_layer = layer_id\n",
    "        repeats = 0\n",
    "        while (last_same_stride_layer < num_layers and\n",
    "               strides[last_same_stride_layer] == strides[layer_id]):\n",
    "            last_same_stride_layer += 1\n",
    "            repeats += 2    # aspect_ratios are added twice per iteration\n",
    "        stride = strides[layer_id]\n",
    "        feature_map_height = input_height // stride\n",
    "        feature_map_width = input_width // stride\n",
    "        for y in range(feature_map_height):\n",
    "            y_center = (y + anchor_offset_y) / feature_map_height\n",
    "            for x in range(feature_map_width):\n",
    "                x_center = (x + anchor_offset_x) / feature_map_width\n",
    "                for _ in range(repeats):\n",
    "                    anchors.append((x_center, y_center))\n",
    "        layer_id = last_same_stride_layer\n",
    "    return np.array(anchors, dtype=np.float32)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    detection_model = \"./face_detection_front.tflite\"\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=detection_model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    print(\"face landmark input info:\",input_details)\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # check the type of the input tensor\n",
    "    floating_model = input_details[0]['dtype'] == np.float32\n",
    "\n",
    "    # NxHxWxC, H:1, W:2\n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "\n",
    "    print(\"height = \",height,\"width = \",width)\n",
    "    image = Image.open(\"./img/face.jpeg\").resize((width, height))\n",
    "    print(image.mode)\n",
    "    print(np.int32(image.getdata()))\n",
    "    if(image.mode == str('RGBA')):\n",
    "      r, g, b, a = image.split()\n",
    "      image = Image.merge(\"RGB\", (r, g, b))\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    print(\"numpy img size = \",image.size)\n",
    "\n",
    "    # add N dim\n",
    "    input_data = np.expand_dims(image, axis=0)\n",
    "\n",
    "    if floating_model:\n",
    "      print(\"float\\n\")\n",
    "      input_data = (np.float32(input_data) -128) / 256\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    results = np.squeeze(output_data)\n",
    "\n",
    "    output_data1= interpreter.get_tensor(output_details[1][\"index\"])\n",
    "\n",
    "    # resutl_file = open(\"./face_detection.txt\",\"w\")\n",
    "    # index = 0\n",
    "    # for point in output_data:\n",
    "    #   if(output_data1[0][index][0] > -10) :\n",
    "    #     resutl_file.writelines(str(point))\n",
    "    #     resutl_file.write(\"\\n\")\n",
    "    #   index += 1\n",
    "    # width == height so scale is the same across the board\n",
    "    scale = interpreter.get_input_details()[0]['shape'][1]\n",
    "    print(\"scale value == \",scale,\"\\n\")\n",
    "    num_points = output_data.shape[-1] // 2\n",
    "    # scale all values (applies to positions, width, and height alike)\n",
    "    boxes = output_data.reshape(-1, num_points, 2) / scale\n",
    "    # adjust center coordinates and key points to anchor positions\n",
    "    boxes[:, 0] += _ssd_generate_anchors(SSD_OPTIONS_FRONT)\n",
    "    for i in range(2, num_points):\n",
    "        boxes[:, i] += _ssd_generate_anchors(SSD_OPTIONS_FRONT)\n",
    "    # convert x_center, y_center, w, h to xmin, ymin, xmax, ymax\n",
    "    center = np.array(boxes[:, 0])\n",
    "    half_size = boxes[:, 1] / 2\n",
    "    boxes[:, 0] = center - half_size\n",
    "    boxes[:, 1] = center + half_size\n",
    "\n",
    "    print(\"boxes value :\\n\",boxes)\n",
    "\n",
    "    first_point = output_data[0][0]\n",
    "    print(first_point)\n",
    "    print(\"scores value :\\n\",output_data1)\n",
    "    draw.rectangle((first_point[0]*128 -  int(first_point[2]/2),first_point[1]*128 -  int(first_point[3]/2),first_point[0]*128 + int(first_point[2]/2),first_point[1]*128 + int(first_point[3]/2)))\n",
    "    image.show(\"face detection\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf # TF2\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  detection_model = \"./traffic_sign_detect.tflite\"\n",
    "\n",
    "  interpreter = tf.lite.Interpreter(model_path=detection_model)\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()\n",
    "  print(\"face landmark input info:\",input_details)\n",
    "  output_details = interpreter.get_output_details()\n",
    "  print(\"face landmark output info:\",output_details)\n",
    "\n",
    "  # check the type of the input tensor\n",
    "  floating_model = input_details[0]['dtype'] == np.float32\n",
    "\n",
    "  # NxHxWxC, H:1, W:2\n",
    "  height = input_details[0]['shape'][1]\n",
    "  width = input_details[0]['shape'][2]\n",
    "\n",
    "  print(\"height = \",height,\"width = \",width)\n",
    "  img = cv2.imread(\"./img/00633.png\")\n",
    "  image = cv2.resize(img,(width,height))\n",
    "\n",
    "  # add N dim\n",
    "  input_data = np.expand_dims(image, axis=0)\n",
    "\n",
    "  if floating_model:\n",
    "    print(\"float\\n\")\n",
    "    input_data = (np.float32(input_data) -128) / 256\n",
    "\n",
    "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "  interpreter.invoke()\n",
    "\n",
    "  boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "  results = np.squeeze(boxes)\n",
    "\n",
    "  classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "\n",
    "  scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "  cv2.imshow(\"source\",img)\n",
    "  print(\"boxes value:\",boxes)\n",
    "  print(\"classes value:\",classes)\n",
    "  print(\"scores value:\",scores)\n",
    "\n",
    "  i = 0\n",
    "  score_arr = scores[0]\n",
    "  for score in score_arr:\n",
    "    if score > 0.6 :\n",
    "      cv2.rectangle(img,(boxes[i][0] * width,boxes[i][1] * height ),(boxes[i][2] * width,boxes[i][3] * width),(255,0,0))\n",
    "    i += 1\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "face landmark input info: [{'name': 'normalized_input_image_tensor', 'index': 412, 'shape': array([  1, 320, 320,   3], dtype=int32), 'shape_signature': array([  1, 320, 320,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "face landmark output info: [{'name': 'TFLite_Detection_PostProcess', 'index': 404, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 405, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 406, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 407, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "height =  320 width =  320\n",
      "float\n",
      "\n",
      "boxes value: [[[ 3.7913254e-01  5.6717750e-03  4.8174575e-01  4.0890500e-02]\n",
      "  [ 6.3266468e-01  2.1366946e-01  7.1255827e-01  2.5851953e-01]\n",
      "  [ 4.9386802e-01  4.7866022e-03  5.6881285e-01  3.2929804e-02]\n",
      "  [ 5.5939019e-01  4.6065426e-01  6.0575676e-01  4.9091244e-01]\n",
      "  [ 4.2628941e-01  4.9165394e-03  5.1562095e-01  4.1266620e-02]\n",
      "  [ 5.3677475e-01  7.9885451e-03  6.0216320e-01  3.0507162e-02]\n",
      "  [ 9.8699331e-04  1.0673404e-02  9.7052765e-01  9.6370786e-01]\n",
      "  [-3.5367224e-01  2.3219651e-01  1.2808498e+00  7.8405792e-01]\n",
      "  [ 1.6325513e-01 -1.8658412e-01  8.3670330e-01  1.1685281e+00]\n",
      "  [ 5.9129804e-01  4.3368900e-01  6.3685471e-01  4.6253395e-01]]]\n",
      "classes value: [[11. 24.  0.  0. 11.  0. 37. 19. 28. 17.]]\n",
      "scores value: [[0.13257122 0.12847346 0.11743629 0.10454491 0.07831344 0.06267607\n",
      "  0.05855235 0.05828506 0.05807903 0.05353376]]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('snpe-env': conda)"
  },
  "interpreter": {
   "hash": "802376ad3ccb937d6ac4cc6b2630e4be47c43798bae6062a8d4b6fe5f1a1b625"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}